{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submit_MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqAYagbZL3Yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "598ca936-731c-4e91-e459-34460d315bad"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZBjT88cLMuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "import nltk\n",
        "from textblob import Word\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def fold_stat(pred_train, pred_test, y_train, y_test):\n",
        "    tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train[:, 1], pred_train).ravel()\n",
        "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test[:, 1], pred_test).ravel()\n",
        "\n",
        "    return tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test\n",
        "\n",
        "def extract_data():\n",
        "    df = pd.read_csv(\"/content/drive/My Drive/COMP4107/data/IMDB Dataset.csv\")\n",
        "    print(\"Doing NLTK\")\n",
        "    df['review'] = df['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "     # remove digits, punctuation and words of less than 3 characters\n",
        "    df['review'] = df['review'].str.replace('[^\\w\\s]', '')\n",
        "    df['review'] = df['review'].str.replace('\\d+', '')  # for digits\n",
        "    df['review'] = df['review'].str.replace(r'(\\b\\w{1,2}\\b)', '')  # for words\n",
        "    df['review'] = df['review'].str.replace(r'\\s+', ' ')\n",
        "    # stop = stopwords.words('english')\n",
        "    # df['review'] = df['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "    df['review'] = df['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "    df.loc[df[\"sentiment\"] == \"positive\", \"sentiment\"] = 1\n",
        "    df.loc[df[\"sentiment\"] == \"negative\", \"sentiment\"] = 0\n",
        "    #\n",
        "    # # store the NLTK processed data into .csv file\n",
        "    # # df.to_csv(\"data/IMDB_modified.csv\", index=False)\n",
        "\n",
        "    # df = pd.read_csv(\"/content/drive/My Drive/COMP4107/data/IMDB_modified.csv\")\n",
        "\n",
        "    # df = pd.read_csv(\"/content/drive/My Drive/COMP4107/data/IMDB Dataset.csv\")\n",
        "    # df.loc[df[\"sentiment\"] == \"positive\", \"sentiment\"] = 1\n",
        "    # df.loc[df[\"sentiment\"] == \"negative\", \"sentiment\"] = 0\n",
        "\n",
        "    df_x = df[\"review\"]\n",
        "    df_y = df[\"sentiment\"]\n",
        "    df_x = np.array(df_x)\n",
        "    df_y = np.array(df_y).astype(float)\n",
        "    # x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=24)\n",
        "\n",
        "    return df_x, df_y\n",
        "\n",
        "\n",
        "def vertorize_and_select(x_train, x_test,y_train, k):\n",
        "    print(\"TF-IDF Vectorize.........\")\n",
        "    cv = TfidfVectorizer(sublinear_tf=True, max_df=0.5, ngram_range=(1, 3))\n",
        "    # cv = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
        "    # cv = CountVectorizer(binary=True, max_df=0.5, ngram_range=(1, 3))\n",
        "    # cv = CountVectorizer(binary=True, max_df=0.5)\n",
        "\n",
        "\n",
        "    x_train_tf = cv.fit_transform(x_train)\n",
        "    x_test_tf = cv.transform(x_test)\n",
        "\n",
        "\n",
        "    print(\"Selecting K Best Features..........\")\n",
        "    selector = SelectKBest(f_classif, k=k)\n",
        "    selector.fit(x_train_tf, y_train)\n",
        "    x_train_tf = selector.transform(x_train_tf)\n",
        "    x_test_tf = selector.transform(x_test_tf)\n",
        "\n",
        "    # transfer the data into array\n",
        "    x_train_tf = x_train_tf.toarray()\n",
        "    x_test_tf = x_test_tf.toarray()\n",
        "    return x_train_tf, x_test_tf\n",
        "\n",
        "\n",
        "def one_hot_encode(y_train, y_test):\n",
        "    return to_categorical(y_train, 2), to_categorical(y_test, 2)\n",
        "\n",
        "\n",
        "def mlp_process_kk(in_size, hidden_neurons_1, out_size):\n",
        "    X = tf.placeholder(\"float\", [None, in_size])\n",
        "    Y = tf.placeholder(\"float\", [None, out_size])\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    # store weight, bia\n",
        "    w_1 = tf.Variable(tf.random.normal([in_size, hidden_neurons_1]))\n",
        "    b_1 = tf.Variable(tf.random.normal([hidden_neurons_1]))\n",
        "    w_o = tf.Variable(tf.random.normal([hidden_neurons_1, out_size]))\n",
        "    b_o = tf.Variable(tf.random.normal([out_size]))\n",
        "\n",
        "    # connect layers\n",
        "    layer_1 = tf.add(tf.matmul(X, w_1), b_1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
        "    out_layer = tf.add(tf.matmul(layer_1, w_o), b_o)\n",
        "    return X, Y, keep_prob, out_layer\n",
        "\n",
        "\n",
        "df_x, df_y = extract_data()\n",
        "# x_train_tf, x_test_tf, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# y_train = to_categorical(y_train, 2)\n",
        "# y_test = to_categorical(y_test, 2)\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=5)\n",
        "sum_acc_test = 0\n",
        "sum_acc_train = 0\n",
        "total_time_cost = 0\n",
        "\n",
        "total_tn_test = 0\n",
        "total_fp_test = 0\n",
        "total_fn_test = 0\n",
        "total_tp_test = 0\n",
        "\n",
        "total_tn_train = 0\n",
        "total_fp_train = 0\n",
        "total_fn_train = 0\n",
        "total_tp_train = 0\n",
        "print(\"=======Evaluating {} Fold Experiment=======\".format(n_splits))\n",
        "count = 0\n",
        "for train, test in kf.split(df_x, df_y):\n",
        "    count += 1\n",
        "    print(\"=======Processing Fold {}=======\".format(count))\n",
        "    x_train = df_x[train]\n",
        "    x_test = df_x[test]\n",
        "    y_train = np.array(df_y[train], dtype=int)\n",
        "    y_test = np.array(df_y[test], dtype=int)\n",
        "    x_train_tf, x_test_tf = vertorize_and_select(x_train, x_test ,y_train, 5000)\n",
        "\n",
        "    y_train = to_categorical(y_train, 2)\n",
        "    y_test = to_categorical(y_test, 2)\n",
        "\n",
        "\n",
        "    feature_size = x_train_tf.shape[1]\n",
        "    label_size = y_train.shape[1]\n",
        "    # X, Y, keep_prob, py_x = mlp_process(feature_size, 64, 64, label_size)\n",
        "    X, Y, keep_prob, py_x = mlp_process_kk(feature_size, 64, label_size)\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
        "    train_op = tf.train.AdamOptimizer().minimize(cost)  # construct an optimizer\n",
        "    predict_op = tf.argmax(py_x, 1)\n",
        "\n",
        "    # saver = tf.train.Saver()\n",
        "\n",
        "    # Launch the graph in a session\n",
        "    \n",
        "    \n",
        "    start = time.process_time() \n",
        "    with tf.Session() as sess:\n",
        "        # you need to initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "        for i in range(3):\n",
        "            for start, end in zip(range(0, len(x_train_tf), 64), range(64, len(x_train_tf) + 1, 64)):\n",
        "                sess.run(train_op, feed_dict={X: x_train_tf[start:end], Y: y_train[start:end], keep_prob: 0.7})\n",
        "\n",
        "            print(\"Iteration {}, validation acc:{}\".format(i+1, np.mean(np.argmax(y_test, axis=1) ==\n",
        "                             sess.run(predict_op, feed_dict={X: x_test_tf, keep_prob: 1}))))\n",
        "\n",
        "\n",
        "        \n",
        "        total_time_cost += time.process_time() - start\n",
        "        pred_train = sess.run(predict_op, feed_dict={X: x_train_tf, keep_prob: 1})\n",
        "        pred_test = sess.run(predict_op, feed_dict={X: x_test_tf, keep_prob: 1})\n",
        "        \n",
        "        y_train_one_hot, y_test_one_hot = y_train, y_test\n",
        "        tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test = fold_stat(pred_train, pred_test,\n",
        "                                                                                               y_train_one_hot,\n",
        "                                                                                               y_test_one_hot)\n",
        "\n",
        "    # tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test = fold_stat(pred_train, pred_test, y_train_one_hot, y_test_one_hot)\n",
        "    total_tn_test += tn_test\n",
        "    total_fp_test += fp_test\n",
        "    total_fn_test += fn_test\n",
        "    total_tp_test += tp_test\n",
        "\n",
        "    total_tn_train += tn_train\n",
        "    total_fp_train += fp_train\n",
        "    total_fn_train += fn_train\n",
        "    total_tp_train += tp_train\n",
        "\n",
        "\n",
        "avg_time = total_time_cost / n_splits\n",
        "\n",
        "avg_tn_train = total_tn_train / n_splits\n",
        "avg_fp_train = total_fp_train / n_splits\n",
        "avg_fn_train = total_fn_train / n_splits\n",
        "avg_tp_train = total_tp_train / n_splits\n",
        "\n",
        "avg_tn_test = total_tn_test / n_splits\n",
        "avg_fp_test = total_fp_test / n_splits\n",
        "avg_fn_test = total_fn_test / n_splits\n",
        "avg_tp_test = total_tp_test / n_splits\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"Train TN: {}\".format(avg_tn_train))\n",
        "print(\"Train FP: {}\".format(avg_fp_train))\n",
        "print(\"Train FN: {}\".format(avg_fn_train))\n",
        "print(\"Train TP: {}\".format(avg_tp_train))\n",
        "# Precision, Recall, f1 score for Test\n",
        "precision_train = avg_tp_train / (avg_tp_train + avg_fp_train)\n",
        "recall_train = avg_tp_train / (avg_tp_train + avg_fn_train)\n",
        "f1_score_train = 2 * ((precision_train * recall_train) / (precision_train + recall_train))\n",
        "print(\"Train Precision: {}\".format(precision_train))\n",
        "print(\"Train Recall: {}\".format(recall_train))\n",
        "print(\"Train F1 Score: {}\".format(f1_score_train))\n",
        "\n",
        "# confusion matrix for Test\n",
        "print(\"----------\")\n",
        "print(\"Test TN: {}\".format(avg_tn_test))\n",
        "print(\"Test FP: {}\".format(avg_fp_test))\n",
        "print(\"Test FN: {}\".format(avg_fn_test))\n",
        "print(\"Test TP: {}\".format(avg_tp_test))\n",
        "# Precision, Recall, f1 score for Test\n",
        "precision_test = avg_tp_test / (avg_tp_test + avg_fp_test)\n",
        "recall_test = avg_tp_test / (avg_tp_test + avg_fn_test)\n",
        "f1_score_test = 2 * ((precision_test * recall_test) / (precision_test + recall_test))\n",
        "print(\"Test Precision: {}\".format(precision_test))\n",
        "print(\"Test Recall: {}\".format(recall_test))\n",
        "print(\"Test F1 Score: {}\".format(f1_score_test))\n",
        "print(\"----------\")\n",
        "# print(\"Time Cost: {}\".format(avg_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}