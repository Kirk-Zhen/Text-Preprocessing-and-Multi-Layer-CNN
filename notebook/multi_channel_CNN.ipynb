{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submit_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHydiq5Sh_nj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff5cd34f-635e-4ee2-de94-83b170b2a184"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fmrSptwga--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "30de22c3-19e7-40e3-db81-064ed717938a"
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Dense, LSTM, Embedding, Input, Flatten, Dropout\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif, SelectKBest\n",
        "import nltk\n",
        "from nltk.corpus import stopwords # dealing with stop words\n",
        "from textblob import Word\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7hOzSZFiHFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fold_stat(pred_train, pred_test, y_train, y_test):\n",
        "    tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train[:, 1], pred_train).ravel()\n",
        "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test[:, 1], pred_test).ravel()\n",
        "\n",
        "    return tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_encode(y_train, y_test):\n",
        "    return to_categorical(y_train, 2), to_categorical(y_test, 2)\n",
        "\n",
        "def multichannel_model(length,vocab_size):\n",
        "    #Channel 1\n",
        "    inputs1 = Input(shape=(length,), name='unigram_input')\n",
        "    embedding1 = Embedding(vocab_size,64, name=\"unigram_embed\")(inputs1)\n",
        "    conv1 = Conv1D(filters=32,kernel_size=1,activation='relu',name = \"unigram_conv1d\")(embedding1)\n",
        "    drop1 = Dropout(0.5, name='unigram_dropout')(conv1)\n",
        "    # pool1 = MaxPooling1D(pool_size=1,name='unigram_pooling')(drop1)\n",
        "    # flat1 = Flatten(name='unigram_flatten')(pool1)\n",
        "    flat1 = Flatten(name='unigram_flatten')(drop1)\n",
        "\n",
        "\n",
        "\n",
        "    inputs2 = Input(shape=(length,), name='bigram_input')\n",
        "    embedding2 = Embedding(vocab_size,64, name=\"bigram_embed\")(inputs2)\n",
        "    conv2 = Conv1D(filters=32,kernel_size=2,activation='relu',name = \"bigram_conv1d\")(embedding2)\n",
        "    drop2 = Dropout(0.5, name='bigram_dropout')(conv2)\n",
        "    # pool2 = MaxPooling1D(pool_size=1,name='bigram_pooling')(drop2)\n",
        "    # flat2 = Flatten(name='bigram_flatten')(pool2)\n",
        "    flat2 = Flatten(name='bigram_flatten')(drop2)\n",
        "\n",
        "    #Channel 3\n",
        "    inputs3 = Input(shape=(length,), name='trigram_input')\n",
        "    embedding3 = Embedding(vocab_size,64, name=\"trigram_embed\")(inputs3)\n",
        "    conv3 = Conv1D(filters=32,kernel_size=3,activation='relu',name = \"trigram_conv1d\")(embedding3)\n",
        "    drop3 = Dropout(0.5, name='trigram_dropout')(conv3)\n",
        "    # pool3 = MaxPooling1D(pool_size=1,name='trigram_pooling')(drop3)\n",
        "    # flat3 = Flatten(name='trigram_flatten')(pool3)\n",
        "    flat3 = Flatten(name='trigram_flatten')(drop3)\n",
        "\n",
        "    #merging channels\n",
        "    merged = concatenate([flat1,flat2,flat3], name=\"feature_combine\")\n",
        "\n",
        "    #interpretation\n",
        "    dense1 = Dense(64,activation='relu', name=\"64-Neurons\")(merged)\n",
        "    outputs = Dense(2,activation='softmax', name=\"Output_Layer\")(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "    #Compiling\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    #Summarize\n",
        "    model.summary()\n",
        "    #plot_model(model,show_shapes=True,to_file = 'multichannel.png')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmcd6H61iNU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/COMP4107/data/IMDB Dataset.csv\")\n",
        "print(\"Doing NLTK\")\n",
        "df['review'] = df['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        " # remove digits, punctuation and words of less than 3 characters\n",
        "df['review'] = df['review'].str.replace('[^\\w\\s]', '')\n",
        "df['review'] = df['review'].str.replace('\\d+', '')  # for digits\n",
        "df['review'] = df['review'].str.replace(r'(\\b\\w{1,2}\\b)', '')  # for words\n",
        "df['review'] = df['review'].str.replace(r'\\s+', ' ')\n",
        "# stop = stopwords.words('english')\n",
        "# df['review'] = df['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df['review'] = df['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "df.loc[df[\"sentiment\"] == \"positive\", \"sentiment\"] = 1\n",
        "df.loc[df[\"sentiment\"] == \"negative\", \"sentiment\"] = 0\n",
        "\n",
        "reviews = df[\"review\"]\n",
        "sentiment = df[\"sentiment\"]\n",
        "\n",
        "max_features = 5000\n",
        "# tokenizer = Tokenizer(num_words=max_features, filters='')\n",
        "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\d+')\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "df_x = tokenizer.texts_to_sequences(reviews)\n",
        "df_x = pad_sequences(df_x)\n",
        "\n",
        "# x_train_tf, x_test_tf, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# y_train = to_categorical(y_train, 2)\n",
        "# y_test = to_categorical(y_test, 2)\n",
        "embed_size = 64\n",
        "length = df_x.shape[1]\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=5)\n",
        "sum_acc_test = 0\n",
        "sum_acc_train = 0\n",
        "total_time_cost = 0\n",
        "\n",
        "total_tn_test = 0\n",
        "total_fp_test = 0\n",
        "total_fn_test = 0\n",
        "total_tp_test = 0\n",
        "\n",
        "total_tn_train = 0\n",
        "total_fp_train = 0\n",
        "total_fn_train = 0\n",
        "total_tp_train = 0\n",
        "print(\"=======Evaluating {} Fold Experiment=======\".format(n_splits))\n",
        "count = 0\n",
        "for train, test in kf.split(df_x, sentiment):\n",
        "    count += 1\n",
        "    x_train_tf = df_x[train]\n",
        "    x_test_tf = df_x[test]\n",
        "    y_train = np.array(sentiment[train], dtype=int)\n",
        "    y_test = np.array(sentiment[test], dtype=int)\n",
        "    y_train = to_categorical(y_train, 2)\n",
        "    y_test = to_categorical(y_test, 2)\n",
        "\n",
        "\n",
        "\n",
        "    model = multichannel_model(length, 5000)\n",
        "\n",
        "    start = time.process_time()\n",
        "\n",
        "    model.fit([x_train_tf, x_train_tf, x_train_tf], y_train,\n",
        "              epochs=3,\n",
        "              batch_size=64,\n",
        "              validation_data=([x_test_tf, x_test_tf, x_test_tf], y_test))\n",
        "\n",
        "    total_time_cost += time.process_time() - start\n",
        "\n",
        "\n",
        "    pred_train = model.predict([x_train_tf, x_train_tf, x_train_tf])\n",
        "    pred_test = model.predict([x_test_tf, x_test_tf, x_test_tf])\n",
        "\n",
        "    pred_train = np.argmax(pred_train, axis=1)\n",
        "    pred_test = np.argmax(pred_test, axis=1)\n",
        "    tn_train, fp_train, fn_train, tp_train, tn_test, fp_test, fn_test, tp_test = fold_stat(pred_train, pred_test, y_train, y_test)\n",
        "\n",
        "    total_tn_test += tn_test\n",
        "    total_fp_test += fp_test\n",
        "    total_fn_test += fn_test\n",
        "    total_tp_test += tp_test\n",
        "\n",
        "    total_tn_train += tn_train\n",
        "    total_fp_train += fp_train\n",
        "    total_fn_train += fn_train\n",
        "    total_tp_train += tp_train\n",
        "\n",
        "avg_tn_train = total_tn_train / n_splits\n",
        "avg_fp_train = total_fp_train / n_splits\n",
        "avg_fn_train = total_fn_train / n_splits\n",
        "avg_tp_train = total_tp_train / n_splits\n",
        "\n",
        "avg_tn_test = total_tn_test / n_splits\n",
        "avg_fp_test = total_fp_test / n_splits\n",
        "avg_fn_test = total_fn_test / n_splits\n",
        "avg_tp_test = total_tp_test / n_splits\n",
        "\n",
        "print(\"----------\")\n",
        "print(\"Train TN: {}\".format(avg_tn_train))\n",
        "print(\"Train FP: {}\".format(avg_fp_train))\n",
        "print(\"Train FN: {}\".format(avg_fn_train))\n",
        "print(\"Train TP: {}\".format(avg_tp_train))\n",
        "# Precision, Recall, f1 score for Test\n",
        "precision_train = avg_tp_train / (avg_tp_train + avg_fp_train)\n",
        "recall_train = avg_tp_train / (avg_tp_train + avg_fn_train)\n",
        "f1_score_train = 2 * ((precision_train * recall_train) / (precision_train + recall_train))\n",
        "print(\"Train Precision: {}\".format(precision_train))\n",
        "print(\"Train Recall: {}\".format(recall_train))\n",
        "print(\"Train F1 Score: {}\".format(f1_score_train))\n",
        "\n",
        "# confusion matrix for Test\n",
        "print(\"----------\")\n",
        "print(\"Test TN: {}\".format(avg_tn_test))\n",
        "print(\"Test FP: {}\".format(avg_fp_test))\n",
        "print(\"Test FN: {}\".format(avg_fn_test))\n",
        "print(\"Test TP: {}\".format(avg_tp_test))\n",
        "# Precision, Recall, f1 score for Test\n",
        "precision_test = avg_tp_test / (avg_tp_test + avg_fp_test)\n",
        "recall_test = avg_tp_test / (avg_tp_test + avg_fn_test)\n",
        "f1_score_test = 2 * ((precision_test * recall_test) / (precision_test + recall_test))\n",
        "print(\"Test Precision: {}\".format(precision_test))\n",
        "print(\"Test Recall: {}\".format(recall_test))\n",
        "print(\"Test F1 Score: {}\".format(f1_score_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}